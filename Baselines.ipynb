{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699238be",
   "metadata": {},
   "source": [
    "# Train/dev split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c38abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8166f5e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts, ids = [], []\n",
    "with open('train_reviews.txt') as f:\n",
    "    for line in f:\n",
    "        text_id, text = line.rstrip('\\r\\n').split('\\t')\n",
    "        texts.append(text)\n",
    "        ids.append(text_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49be4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_texts, dev_texts, train_ids, dev_ids = train_test_split(texts, ids, random_state=69, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a7838ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_aspects, dev_aspects = [], []\n",
    "with open('train_aspects.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\r\\n')\n",
    "        text_id = line.split('\\t')[0]\n",
    "        if text_id in train_ids:\n",
    "            train_aspects.append(line)\n",
    "        if text_id in dev_ids:\n",
    "            dev_aspects.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df43cf05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sentiment, dev_sentiment = [], []\n",
    "with open('train_cats.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\r\\n')\n",
    "        text_id = line.split('\\t')[0]\n",
    "        if text_id in train_ids:\n",
    "            train_sentiment.append(line)\n",
    "        if text_id in dev_ids:\n",
    "            dev_sentiment.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47be0f98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('train_split_aspects.txt', 'w') as f:\n",
    "    for l in train_aspects:\n",
    "        print(l, file=f)\n",
    "with open('valid_aspects.txt', 'w') as f:\n",
    "    for l in dev_aspects:\n",
    "        print(l, file=f)\n",
    "with open('train_split_reviews.txt', 'w') as f:\n",
    "    for i, l in zip(train_ids, train_texts):\n",
    "        print(i, l, sep=\"\\t\", file=f)\n",
    "with open('dev_reviews.txt', 'w') as f:\n",
    "    for i, l in zip(dev_ids, dev_texts):\n",
    "        print(i, l, sep=\"\\t\", file=f)\n",
    "with open('train_split_cats.txt', 'w') as f:\n",
    "    for l in train_sentiment:\n",
    "        print(l, file=f)\n",
    "with open('dev_cats.txt', 'w') as f:\n",
    "    for l in dev_sentiment:\n",
    "        print(l, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0e2817",
   "metadata": {},
   "source": [
    "# Baseline 1,2: категория и тональность упоминаний\n",
    "\n",
    "Выделяем только аспекты, встретившиеся в train'е, приписываем самую частотную категорию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87bcfc14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20d96f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_asp = pd.read_csv(\n",
    "    'train_split_aspects.txt', \n",
    "    delimiter='\\t', \n",
    "    names=['text_id', 'category', 'mention', 'start', 'end', 'sentiment']\n",
    ")\n",
    "train_texts = pd.read_csv('train_split_reviews.txt', delimiter='\\t', names=['text_id','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4673f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c3ff611a60479bba36ae30dca163b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 21:44:22 INFO: Downloading default packages for language: ru (Russian) ...\n",
      "2023-12-28 21:44:23 INFO: File exists: /home/jovyan/stanza_resources/ru/default.zip\n",
      "2023-12-28 21:44:29 INFO: Finished downloading models and saved to /home/jovyan/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "335a71e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 21:44:29 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a2218b934645f6b5364104388a646a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 21:44:29 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "==================================\n",
      "\n",
      "2023-12-28 21:44:29 INFO: Using device: cuda\n",
      "2023-12-28 21:44:29 INFO: Loading: tokenize\n",
      "2023-12-28 21:44:30 INFO: Loading: lemma\n",
      "2023-12-28 21:44:31 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('ru', processors='tokenize,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8237cc2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    doc = nlp(text)\n",
    "    words = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42b115a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce707ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_asp['norm_mention'] = [tuple(normalize(m)) for m in train_asp['mention']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b87916",
   "metadata": {},
   "source": [
    "Строим частотный словарь \"токенизированное упоминание + категория\"\n",
    "\n",
    "Категория - аспектная категория или тональность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89e2d282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mention_category(data, cat_type):\n",
    "    mention_categories = data.value_counts(subset=['norm_mention', cat_type])\n",
    "    mention_categories_dict = defaultdict(dict)\n",
    "    for key, value in mention_categories.items():\n",
    "        mention_categories_dict[key[0]][key[1]] = value\n",
    "    return {k: Counter(v).most_common(1)[0][0] for k, v in mention_categories_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0392994a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_mention_cat = get_mention_category(train_asp, 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a87cad27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_mention_sentiment = get_mention_category(train_asp, 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7509525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev_texts = pd.read_csv('dev_reviews.txt', delimiter='\\t', names=['text_id', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e6faa",
   "metadata": {},
   "source": [
    "Длины упоминаний аспектов в трейне:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a37fc436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 591, 2: 321, 3: 158, 4: 61, 5: 24, 6: 12, 7: 5, 9: 2, 10: 1, 8: 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([len(x) for x in best_mention_sentiment.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec215e94",
   "metadata": {},
   "source": [
    "Будем учитывать только упоминания длиной 1-5 токенов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "746d73ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def label_texts(text, mentions, sentiments, max_len=5):\n",
    "    tokenized = [word for sent in nlp(text).sentences for word in sent.words]\n",
    "    text_end = len(tokenized)\n",
    "    for i, token in enumerate(tokenized):\n",
    "        for l in reversed(range(max_len)):\n",
    "            if i + l > text_end:\n",
    "                continue\n",
    "            span = tokenized[i:i + l]\n",
    "            key = tuple([t.lemma for t in span])\n",
    "            if key in mentions:\n",
    "                start, end = span[0].start_char, span[-1].end_char\n",
    "                yield mentions[key], text[start:end], start, end, sentiments[key]\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c65ad",
   "metadata": {},
   "source": [
    "Применяем частотные данные к текстам из dev:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7acb5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('valid_true_base_aspects.txt', 'w') as f:\n",
    "    for text, idx in zip(dev_texts['text'], dev_texts['text_id']):\n",
    "        for asp in label_texts(text, best_mention_cat, best_mention_sentiment):\n",
    "            print(idx, *asp, sep=\"\\t\", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f0671",
   "metadata": {},
   "source": [
    "# Baseline 3\n",
    "\n",
    "Посчитаем упоминания аспектов с предсказанной тональностью, припишем\n",
    "- `absence` - если нет упоминаний данной категории\n",
    "- `both` - если есть упоминания с разной тональностью\n",
    "- `positive/neutral/negative` - если все упоминания одной тональности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02a2fa13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CATEGORIES = ['Whole', 'Interior', 'Service', 'Food', 'Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e263354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_full_sentiment(text, mentions, sentiment, max_len=5):\n",
    "    asp_counter = defaultdict(Counter)\n",
    "    for asp in label_texts(text, best_mention_cat, best_mention_sentiment, max_len):\n",
    "        category, *_, sentiment = asp\n",
    "        asp_counter[category][sentiment] += 1\n",
    "    for c in CATEGORIES:\n",
    "        if not asp_counter[c]:\n",
    "            s = 'absence'\n",
    "        elif len(asp_counter[c]) == 1:\n",
    "            s = asp_counter[c].most_common(1)[0][0]\n",
    "        else:\n",
    "            s = 'both'\n",
    "        yield c, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4e7de86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('valid_true_base_cats.txt', 'w') as f:\n",
    "    for text, idx in zip(dev_texts['text'], dev_texts['text_id']):\n",
    "        for c, s in get_full_sentiment(text, best_mention_cat, best_mention_sentiment):\n",
    "            print(idx, c, s, sep=\"\\t\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dc17642",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full match precision: 0.4146224146224146\n",
      "Full match recall: 0.737382378100941\n",
      "Partial match ratio in pred: 0.5108225108225108\n",
      "Full category accuracy: 0.4078884078884079\n",
      "Partial category accuracy: 0.5036075036075036\n",
      "Patial sentiment accuracy: 0.605\n",
      "Full sentiment accuracy: 0.665893271461717\n",
      "Overall sentiment accuracy: 0.5492957746478874\n"
     ]
    }
   ],
   "source": [
    "from inference import reference_check\n",
    "\n",
    "reference_check(\n",
    "    \"dev_aspects.txt\", \"valid_true_base_aspects.txt\",\n",
    "    \"dev_cats.txt\", \"valid_true_base_cats.txt\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
